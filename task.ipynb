{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.layers.python.layers import layers\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "from utils import make_dir, read_or_create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_codones(sseq):\n",
    "    crop = len(sseq) % 3\n",
    "    cropped_seq = sseq[:-crop] if crop > 0 else sseq\n",
    "\n",
    "    return [cropped_seq[i:i+3] for i in range(0, len(cropped_seq), 3)]\n",
    "\n",
    "def seq_to3(seq):\n",
    "    return [make_codones(seq[i:]) for i in range(3)]\n",
    "\n",
    "def create_all_codones(df):\n",
    "    codones = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i, :][0]\n",
    "        codones.extend(seq_to3(row))\n",
    "        \n",
    "    return codones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df = pd.read_table(\"data/family_classification_sequences.tab\")\n",
    "seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codones = read_or_create(read_path=\"data/all_codones.pickle\",\n",
    "                             producer=lambda: create_all_codones(seq_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(index_words_list, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model \"\"\"\n",
    "    \n",
    "    for index_words in index_words_list:\n",
    "        for index, center in enumerate(index_words):\n",
    "            context = random.randint(1, context_window_size)\n",
    " \n",
    "            # get a random target before the center word\n",
    "            for target in index_words[max(0, index - context): index]:\n",
    "                yield center, target\n",
    "                \n",
    "            # get a random target after the center wrod\n",
    "            for target in index_words[index + 1: index + context + 1]:\n",
    "                yield center, target\n",
    "\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays \"\"\"\n",
    "    \n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1], dtype=np.int32)\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        yield center_batch, target_batch\n",
    "\n",
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]\n",
    "\n",
    "def cod_to_dict(cod, dictionary):\n",
    "    return [dictionary[key] for key in cod]\n",
    "\n",
    "def make_dictionary(all_codones):\n",
    "    counter = Counter(flatten(all_codones))\n",
    "    ordered = map(lambda it: it[0], counter.most_common())\n",
    "    dictionary = {cod: i for i, cod in enumerate(ordered)}\n",
    "    return dictionary\n",
    "\n",
    "def process_data(all_codones, dictionary, batch_size, skip_window):\n",
    "    cod_dicts = (cod_to_dict(cod, dictionary) for cod in all_codones)\n",
    "    single_gen = generate_sample(cod_dicts, context_window_size=skip_window)\n",
    "    return get_batch(single_gen, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = make_dictionary(all_codones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SKIP_WINDOW = 12  # the context window\n",
    "\n",
    "batch_gen = process_data(all_codones, dictionary, BATCH_SIZE, SKIP_WINDOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGramModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazy import lazy\n",
    "\n",
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.lr = learning_rate\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name=\"global_step\")\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    @lazy\n",
    "    def placeholders(self):\n",
    "        with tf.name_scope(\"data\"):\n",
    "            center_words = tf.placeholder(tf.int32, shape=[self.batch_size], name=\"center_words\")\n",
    "            target_words = tf.placeholder(tf.int32, shape=[self.batch_size, 1], name=\"target_words\")\n",
    "            return center_words, target_words\n",
    "    \n",
    "    @lazy\n",
    "    def embedding(self):\n",
    "        with tf.name_scope(\"embed\"):\n",
    "            return tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size], 0, 2), name=\"embed_matrix\")\n",
    "    \n",
    "    @lazy\n",
    "    def loss(self):\n",
    "        center_words, target_words = self.placeholders\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            embed = tf.nn.embedding_lookup(self.embedding, center_words, name=\"embed\")\n",
    "            \n",
    "            # construct variables for out hidden layer\n",
    "            layer = layers.fully_connected(inputs=embed,\n",
    "                                           num_outputs=self.embed_size,\n",
    "                                           weights_initializer=tf.truncated_normal_initializer(mean=0, \n",
    "                                                                                               stddev=1.0 / (self.embed_size ** 0.5)),\n",
    "                                           biases_initializer=tf.constant_initializer(value=0.1))\n",
    "            \n",
    "            # construct variables for NCE loss\n",
    "            nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size],\n",
    "                                                         stddev=1.0 / (self.embed_size ** 0.5)),\n",
    "                                     name=\"nce_weight\")\n",
    "            nce_bias = tf.Variable(tf.zeros([self.vocab_size]), name=\"nce_bias\")\n",
    "\n",
    "            # define loss function to be NCE loss function\n",
    "            return tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                 biases=nce_bias,\n",
    "                                                 labels=target_words,\n",
    "                                                 inputs=layer,\n",
    "                                                 num_sampled=self.num_sampled,\n",
    "                                                 num_classes=self.vocab_size), name=\"loss\")\n",
    "    \n",
    "    @lazy\n",
    "    def optimizer(self):\n",
    "        return tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "    @lazy\n",
    "    def summary(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "            return tf.summary.merge_all()\n",
    "        \n",
    "    def build(self):\n",
    "        self.optimizer\n",
    "        self.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 9424\n",
    "EMBED_SIZE = 100 # dimension of the word embedding vectors\n",
    "NUM_SAMPLED = 5  # Number of negative examples to sample.\n",
    "LEARNING_RATE = 0.9\n",
    "NUM_TRAIN_STEPS = 200000\n",
    "SKIP_STEP = 2000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    model = SkipGramModel(VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "    model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import make_dir\n",
    "\n",
    "def train_model(model, batch_gen, num_train_steps, learning_rate, skip_step):\n",
    "    make_dir(\"checkpoints\")\n",
    " \n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "    \n",
    "    with tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname(\"checkpoints/checkpoint\"))\n",
    "\n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and os.path.isfile(ckpt.model_checkpoint_path):\n",
    "            model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        total_loss = 0.0  # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter(\"improved_graph/lr\" + str(learning_rate), sess.graph)\n",
    "        \n",
    "        initial_step = model.global_step.eval()\n",
    "        for index in range(initial_step, initial_step + num_train_steps):\n",
    "            centers, targets = next(batch_gen)\n",
    "            \n",
    "            center_words, target_words = model.placeholders\n",
    "            feed_dict = {center_words: centers, target_words: targets}\n",
    "            loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary],\n",
    "                                              feed_dict=feed_dict)\n",
    "            \n",
    "            writer.add_summary(summary, global_step=index)\n",
    "            \n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % skip_step == 0:\n",
    "                print(\"Average loss at step {:>5}: {:5.1f}\".format(index, total_loss / skip_step))\n",
    "                total_loss = 0.0\n",
    "                model.saver.save(sess, \"checkpoints/skip-gram\", index)\n",
    "\n",
    "        final_embed_matrix = sess.run(model.embedding)\n",
    "        return final_embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embed_matrix = train_model(model, batch_gen, NUM_TRAIN_STEPS, LEARNING_RATE, SKIP_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_embeded = tsne.fit_transform(final_embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(X_embeded, columns=[\"x0\", \"x1\"])\n",
    "unique_codones = sorted(dictionary, key=dictionary.get)\n",
    "tsne_df[\"codone\"] = list(unique_codones)\n",
    "tsne_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"data/acid_properties.csv\"\n",
    "props = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acid_dict(some_c, props):\n",
    "    prop_by_letter = [props[props.acid == let].iloc[:, 1:] for let in some_c]   \n",
    "    df_concat = pd.concat(prop_by_letter)\n",
    "    res = df_concat.mean()\n",
    "    dres = dict(res)\n",
    "    dres[\"acid\"] = some_c\n",
    "    return dres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"data/all_acid_dicts.pickle\"\n",
    "producer = lambda: [acid_dict(some_c, props) for some_c in tsne_df.codone]\n",
    "all_acid_dicts = read_or_create(save_path, producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acid_df = pd.DataFrame(all_acid_dicts)\n",
    "all_acid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = all_acid_df.join(tsne_df.set_index(\"codone\"), on=\"acid\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_embedding_properties(final_df):\n",
    "    \"\"\" Plot properties of our acids \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(25, 20))\n",
    "    for i, p in enumerate([\"hydrophobicity\", \"mass\", \"number_of_atoms\", \"volume\"]):\n",
    "        plt.subplot(2,2,i+1)\n",
    "        plt.title(p, fontsize=25)\n",
    "        plt.scatter(final_df.x0, final_df.x1, c=final_df[p], s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding_properties(final_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
