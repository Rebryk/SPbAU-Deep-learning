{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: fill empty spaces in the following agent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, name, state_size, action_size, learning_rate = 0.002):\n",
    "        self.name = name\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, [None, state_size])\n",
    "        self.y = tf.placeholder(tf.float32, [None, action_size])\n",
    "        \n",
    "        self.y_pred, self.optimizer = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            layer_1 = slim.fully_connected(self.X, 25)\n",
    "            layer_2 = slim.fully_connected(layer_1, 25)\n",
    "            y_pred = slim.fully_connected(layer_2, self.action_size, activation_fn=None)\n",
    "            loss = tf.losses.mean_squared_error(self.y, y_pred)\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
    "            return y_pred, optimizer\n",
    "    \n",
    "    def predict_reward(self, sess, state):\n",
    "        return sess.run(self.y_pred, feed_dict={self.X: state})[0]\n",
    "    \n",
    "    def predict_action(self, sess, state):\n",
    "        return np.argmax(self.predict_reward(sess, state))\n",
    "    \n",
    "    def train(self, sess, X, y):\n",
    "        sess.run(self.optimizer, feed_dict={self.X: X, self.y: y})\n",
    "    \n",
    "class DeepQAgent:\n",
    "    def __init__(self, state_size, action_size, render=True):\n",
    "        self.render = render\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / 50000\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        \n",
    "        # replay memory\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "        self.model = Model('model', state_size, action_size)\n",
    "        self.target_model = Model('target_model', state_size, action_size)\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter('./graphs', self.sess.graph)\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update your target model to the model you are currently learning at regular time intervals\"\"\"\n",
    "        \n",
    "        vars_1 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'target_model')\n",
    "        vars_2 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'model')\n",
    "\n",
    "        for x, y in zip(vars_1, vars_2):\n",
    "            self.sess.run(tf.assign(x, y.eval()))\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"The choice of action uses the epsilon-greedy policy for the current network.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return self.model.predict_action(self.sess, state)\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save <s, a, r, s'> to replay_memory\"\"\"\n",
    "        if action == 2:\n",
    "            action = 1\n",
    "            \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "    \n",
    "    def train_replay(self):\n",
    "        \"\"\"Random sampling of batch_size samples from replay memory\"\"\"\n",
    "        \n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.action_size))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            target = self.model.predict_reward(self.sess, state)\n",
    "\n",
    "            # As in queuing, it gets the maximum Q Value at s'. However, it is imported from the target model.\n",
    "            if done:\n",
    "                target[action] = 100\n",
    "                # target[action] = reward\n",
    "            else:\n",
    "                max_q = np.max(self.target_model.predict_reward(self.sess, next_state))\n",
    "                target[action] = reward + self.discount_factor * max_q\n",
    "            \n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        self.model.train(self.sess, update_input, update_target)\n",
    "    \n",
    "    def load_model(self):\n",
    "        try:\n",
    "            print('Trying to restore last checkpoint...')\n",
    "            last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=\"./checkpoints/\")\n",
    "            self.saver.restore(self.sess, save_path=last_chk_path)\n",
    "            print('Restored checkpoint from:', last_chk_path)\n",
    "        except:\n",
    "            print('Failed to restore checkpoint. Initializing variables instead.')\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def save_model(self, name):\n",
    "        print('Saving model...')\n",
    "        self.saver.save(self.sess, save_path='checkpoints/' + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0] # should be equal 2\n",
    "\n",
    "ACTION_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "agent = DeepQAgent(state_size, ACTION_SIZE, render=False)\n",
    "scores, episodes = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1   score: -200.0   memory length: 200   epsilon: 0.9960200000000077\n",
      "episode: 2   score: -200.0   memory length: 400   epsilon: 0.9920400000000154\n",
      "episode: 3   score: -200.0   memory length: 600   epsilon: 0.988060000000023\n",
      "episode: 4   score: -200.0   memory length: 800   epsilon: 0.9840800000000307\n",
      "episode: 5   score: -200.0   memory length: 1000   epsilon: 0.9801000000000384\n",
      "episode: 6   score: -200.0   memory length: 1200   epsilon: 0.9761200000000461\n",
      "episode: 7   score: -200.0   memory length: 1400   epsilon: 0.9721400000000537\n",
      "episode: 8   score: -200.0   memory length: 1600   epsilon: 0.9681600000000614\n",
      "episode: 9   score: -200.0   memory length: 1800   epsilon: 0.9641800000000691\n",
      "episode: 10   score: -200.0   memory length: 2000   epsilon: 0.9602000000000768\n",
      "episode: 11   score: -200.0   memory length: 2200   epsilon: 0.9562200000000844\n",
      "episode: 12   score: -185.0   memory length: 2385   epsilon: 0.9525385000000915\n",
      "episode: 13   score: -200.0   memory length: 2585   epsilon: 0.9485585000000992\n",
      "episode: 14   score: -200.0   memory length: 2785   epsilon: 0.9445785000001069\n",
      "episode: 15   score: -200.0   memory length: 2985   epsilon: 0.9405985000001146\n",
      "episode: 16   score: -200.0   memory length: 3185   epsilon: 0.9366185000001223\n",
      "episode: 17   score: -200.0   memory length: 3385   epsilon: 0.9326385000001299\n",
      "episode: 18   score: -200.0   memory length: 3585   epsilon: 0.9286585000001376\n",
      "episode: 19   score: -200.0   memory length: 3785   epsilon: 0.9246785000001453\n",
      "episode: 20   score: -200.0   memory length: 3985   epsilon: 0.920698500000153\n",
      "episode: 21   score: -200.0   memory length: 4185   epsilon: 0.9167185000001606\n",
      "episode: 22   score: -200.0   memory length: 4385   epsilon: 0.9127385000001683\n",
      "episode: 23   score: -200.0   memory length: 4585   epsilon: 0.908758500000176\n",
      "episode: 24   score: -200.0   memory length: 4785   epsilon: 0.9047785000001837\n",
      "episode: 25   score: -200.0   memory length: 4985   epsilon: 0.9007985000001913\n",
      "Saving model...\n",
      "episode: 26   score: -200.0   memory length: 5185   epsilon: 0.896818500000199\n",
      "episode: 27   score: -200.0   memory length: 5385   epsilon: 0.8928385000002067\n",
      "episode: 28   score: -200.0   memory length: 5585   epsilon: 0.8888585000002144\n",
      "episode: 29   score: -200.0   memory length: 5785   epsilon: 0.8848785000002221\n",
      "episode: 30   score: -200.0   memory length: 5985   epsilon: 0.8808985000002297\n",
      "episode: 31   score: -200.0   memory length: 6185   epsilon: 0.8769185000002374\n",
      "episode: 32   score: -200.0   memory length: 6385   epsilon: 0.8729385000002451\n",
      "episode: 33   score: -200.0   memory length: 6585   epsilon: 0.8689585000002528\n",
      "episode: 34   score: -200.0   memory length: 6785   epsilon: 0.8649785000002604\n",
      "episode: 35   score: -200.0   memory length: 6985   epsilon: 0.8609985000002681\n",
      "episode: 36   score: -200.0   memory length: 7185   epsilon: 0.8570185000002758\n",
      "episode: 37   score: -200.0   memory length: 7385   epsilon: 0.8530385000002835\n",
      "episode: 38   score: -200.0   memory length: 7585   epsilon: 0.8490585000002911\n",
      "episode: 39   score: -200.0   memory length: 7785   epsilon: 0.8450785000002988\n",
      "episode: 40   score: -200.0   memory length: 7985   epsilon: 0.8410985000003065\n",
      "episode: 41   score: -200.0   memory length: 8185   epsilon: 0.8371185000003142\n",
      "episode: 42   score: -200.0   memory length: 8385   epsilon: 0.8331385000003219\n",
      "episode: 43   score: -200.0   memory length: 8585   epsilon: 0.8291585000003295\n",
      "episode: 44   score: -200.0   memory length: 8785   epsilon: 0.8251785000003372\n",
      "episode: 45   score: -200.0   memory length: 8985   epsilon: 0.8211985000003449\n",
      "episode: 46   score: -200.0   memory length: 9185   epsilon: 0.8172185000003526\n",
      "episode: 47   score: -200.0   memory length: 9385   epsilon: 0.8132385000003602\n",
      "episode: 48   score: -200.0   memory length: 9585   epsilon: 0.8092585000003679\n",
      "episode: 49   score: -200.0   memory length: 9785   epsilon: 0.8052785000003756\n",
      "episode: 50   score: -200.0   memory length: 9985   epsilon: 0.8012985000003833\n",
      "Saving model...\n",
      "episode: 51   score: -200.0   memory length: 10000   epsilon: 0.797318500000391\n",
      "episode: 52   score: -200.0   memory length: 10000   epsilon: 0.7933385000003986\n",
      "episode: 53   score: -200.0   memory length: 10000   epsilon: 0.7893585000004063\n",
      "episode: 54   score: -200.0   memory length: 10000   epsilon: 0.785378500000414\n",
      "episode: 55   score: -200.0   memory length: 10000   epsilon: 0.7813985000004217\n",
      "episode: 56   score: -200.0   memory length: 10000   epsilon: 0.7774185000004293\n",
      "episode: 57   score: -200.0   memory length: 10000   epsilon: 0.773438500000437\n",
      "episode: 58   score: -200.0   memory length: 10000   epsilon: 0.7694585000004447\n",
      "episode: 59   score: -200.0   memory length: 10000   epsilon: 0.7654785000004524\n",
      "episode: 60   score: -200.0   memory length: 10000   epsilon: 0.76149850000046\n",
      "episode: 61   score: -200.0   memory length: 10000   epsilon: 0.7575185000004677\n",
      "episode: 62   score: -200.0   memory length: 10000   epsilon: 0.7535385000004754\n",
      "episode: 63   score: -200.0   memory length: 10000   epsilon: 0.7495585000004831\n",
      "episode: 64   score: -200.0   memory length: 10000   epsilon: 0.7455785000004908\n",
      "episode: 65   score: -200.0   memory length: 10000   epsilon: 0.7415985000004984\n",
      "episode: 66   score: -200.0   memory length: 10000   epsilon: 0.7376185000005061\n",
      "episode: 67   score: -200.0   memory length: 10000   epsilon: 0.7336385000005138\n",
      "episode: 68   score: -200.0   memory length: 10000   epsilon: 0.7296585000005215\n",
      "episode: 69   score: -200.0   memory length: 10000   epsilon: 0.7256785000005291\n",
      "episode: 70   score: -200.0   memory length: 10000   epsilon: 0.7216985000005368\n",
      "episode: 71   score: -200.0   memory length: 10000   epsilon: 0.7177185000005445\n",
      "episode: 72   score: -200.0   memory length: 10000   epsilon: 0.7137385000005522\n",
      "episode: 73   score: -200.0   memory length: 10000   epsilon: 0.7097585000005598\n",
      "episode: 74   score: -200.0   memory length: 10000   epsilon: 0.7057785000005675\n",
      "episode: 75   score: -200.0   memory length: 10000   epsilon: 0.7017985000005752\n",
      "Saving model...\n",
      "episode: 76   score: -200.0   memory length: 10000   epsilon: 0.6978185000005829\n",
      "episode: 77   score: -200.0   memory length: 10000   epsilon: 0.6938385000005906\n",
      "episode: 78   score: -200.0   memory length: 10000   epsilon: 0.6898585000005982\n",
      "episode: 79   score: -200.0   memory length: 10000   epsilon: 0.6858785000006059\n",
      "episode: 80   score: -200.0   memory length: 10000   epsilon: 0.6818985000006136\n",
      "episode: 81   score: -128.0   memory length: 10000   epsilon: 0.6793513000006185\n",
      "episode: 82   score: -200.0   memory length: 10000   epsilon: 0.6753713000006262\n",
      "episode: 83   score: -200.0   memory length: 10000   epsilon: 0.6713913000006339\n",
      "episode: 84   score: -200.0   memory length: 10000   epsilon: 0.6674113000006415\n",
      "episode: 85   score: -200.0   memory length: 10000   epsilon: 0.6634313000006492\n",
      "episode: 86   score: -200.0   memory length: 10000   epsilon: 0.6594513000006569\n",
      "episode: 87   score: -200.0   memory length: 10000   epsilon: 0.6554713000006646\n",
      "episode: 88   score: -200.0   memory length: 10000   epsilon: 0.6514913000006722\n",
      "episode: 89   score: -200.0   memory length: 10000   epsilon: 0.6475113000006799\n",
      "episode: 90   score: -200.0   memory length: 10000   epsilon: 0.6435313000006876\n",
      "episode: 91   score: -200.0   memory length: 10000   epsilon: 0.6395513000006953\n",
      "episode: 92   score: -200.0   memory length: 10000   epsilon: 0.6355713000007029\n",
      "episode: 93   score: -200.0   memory length: 10000   epsilon: 0.6315913000007106\n",
      "episode: 94   score: -200.0   memory length: 10000   epsilon: 0.6276113000007183\n",
      "episode: 95   score: -200.0   memory length: 10000   epsilon: 0.623631300000726\n",
      "episode: 96   score: -200.0   memory length: 10000   epsilon: 0.6196513000007337\n",
      "episode: 97   score: -200.0   memory length: 10000   epsilon: 0.6156713000007413\n",
      "episode: 98   score: -200.0   memory length: 10000   epsilon: 0.611691300000749\n",
      "episode: 99   score: -200.0   memory length: 10000   epsilon: 0.6077113000007567\n",
      "episode: 100   score: -200.0   memory length: 10000   epsilon: 0.6037313000007644\n",
      "Saving model...\n",
      "episode: 101   score: -200.0   memory length: 10000   epsilon: 0.599751300000772\n",
      "episode: 102   score: -200.0   memory length: 10000   epsilon: 0.5957713000007797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 103   score: -200.0   memory length: 10000   epsilon: 0.5917913000007874\n",
      "episode: 104   score: -200.0   memory length: 10000   epsilon: 0.5878113000007951\n",
      "episode: 105   score: -200.0   memory length: 10000   epsilon: 0.5838313000008027\n",
      "episode: 106   score: -200.0   memory length: 10000   epsilon: 0.5798513000008104\n",
      "episode: 107   score: -200.0   memory length: 10000   epsilon: 0.5758713000008181\n",
      "episode: 108   score: -200.0   memory length: 10000   epsilon: 0.5718913000008258\n",
      "episode: 109   score: -200.0   memory length: 10000   epsilon: 0.5679113000008335\n",
      "episode: 110   score: -200.0   memory length: 10000   epsilon: 0.5639313000008411\n",
      "episode: 111   score: -200.0   memory length: 10000   epsilon: 0.5599513000008488\n",
      "episode: 112   score: -200.0   memory length: 10000   epsilon: 0.5559713000008565\n",
      "episode: 113   score: -200.0   memory length: 10000   epsilon: 0.5519913000008642\n",
      "episode: 114   score: -200.0   memory length: 10000   epsilon: 0.5480113000008718\n",
      "episode: 115   score: -200.0   memory length: 10000   epsilon: 0.5440313000008795\n",
      "episode: 116   score: -200.0   memory length: 10000   epsilon: 0.5400513000008872\n",
      "episode: 117   score: -200.0   memory length: 10000   epsilon: 0.5360713000008949\n",
      "episode: 118   score: -200.0   memory length: 10000   epsilon: 0.5320913000009025\n",
      "episode: 119   score: -200.0   memory length: 10000   epsilon: 0.5281113000009102\n",
      "episode: 120   score: -200.0   memory length: 10000   epsilon: 0.5241313000009179\n",
      "episode: 121   score: -200.0   memory length: 10000   epsilon: 0.5201513000009256\n",
      "episode: 122   score: -200.0   memory length: 10000   epsilon: 0.5161713000009333\n",
      "episode: 123   score: -200.0   memory length: 10000   epsilon: 0.5121913000009409\n",
      "episode: 124   score: -200.0   memory length: 10000   epsilon: 0.5082113000009486\n",
      "episode: 125   score: -200.0   memory length: 10000   epsilon: 0.5042313000009563\n",
      "Saving model...\n",
      "episode: 126   score: -200.0   memory length: 10000   epsilon: 0.500251300000964\n",
      "episode: 127   score: -200.0   memory length: 10000   epsilon: 0.4962713000009612\n",
      "episode: 128   score: -200.0   memory length: 10000   epsilon: 0.4922913000009578\n",
      "episode: 129   score: -200.0   memory length: 10000   epsilon: 0.48831130000095435\n",
      "episode: 130   score: -200.0   memory length: 10000   epsilon: 0.4843313000009509\n",
      "episode: 131   score: -200.0   memory length: 10000   epsilon: 0.4803513000009475\n",
      "episode: 132   score: -200.0   memory length: 10000   epsilon: 0.4763713000009441\n",
      "episode: 133   score: -200.0   memory length: 10000   epsilon: 0.47239130000094065\n",
      "episode: 134   score: -200.0   memory length: 10000   epsilon: 0.4684113000009372\n",
      "episode: 135   score: -200.0   memory length: 10000   epsilon: 0.4644313000009338\n",
      "episode: 136   score: -200.0   memory length: 10000   epsilon: 0.4604513000009304\n",
      "episode: 137   score: -200.0   memory length: 10000   epsilon: 0.45647130000092695\n",
      "episode: 138   score: -200.0   memory length: 10000   epsilon: 0.4524913000009235\n",
      "episode: 139   score: -200.0   memory length: 10000   epsilon: 0.4485113000009201\n",
      "episode: 140   score: -200.0   memory length: 10000   epsilon: 0.4445313000009167\n",
      "episode: 141   score: -200.0   memory length: 10000   epsilon: 0.44055130000091325\n",
      "episode: 142   score: -200.0   memory length: 10000   epsilon: 0.4365713000009098\n",
      "episode: 143   score: -200.0   memory length: 10000   epsilon: 0.4325913000009064\n",
      "episode: 144   score: -200.0   memory length: 10000   epsilon: 0.428611300000903\n",
      "episode: 145   score: -200.0   memory length: 10000   epsilon: 0.42463130000089955\n",
      "episode: 146   score: -200.0   memory length: 10000   epsilon: 0.4206513000008961\n",
      "episode: 147   score: -200.0   memory length: 10000   epsilon: 0.4166713000008927\n",
      "episode: 148   score: -200.0   memory length: 10000   epsilon: 0.41269130000088927\n",
      "episode: 149   score: -147.0   memory length: 10000   epsilon: 0.40976600000088675\n",
      "episode: 150   score: -200.0   memory length: 10000   epsilon: 0.40578600000088333\n",
      "Saving model...\n",
      "episode: 151   score: -150.0   memory length: 10000   epsilon: 0.40280100000088076\n",
      "episode: 152   score: -200.0   memory length: 10000   epsilon: 0.39882100000087733\n",
      "episode: 153   score: -200.0   memory length: 10000   epsilon: 0.3948410000008739\n",
      "episode: 154   score: -94.0   memory length: 10000   epsilon: 0.3929704000008723\n",
      "episode: 155   score: -126.0   memory length: 10000   epsilon: 0.39046300000087014\n",
      "episode: 156   score: -200.0   memory length: 10000   epsilon: 0.3864830000008667\n",
      "episode: 157   score: -138.0   memory length: 10000   epsilon: 0.38373680000086435\n",
      "episode: 158   score: -200.0   memory length: 10000   epsilon: 0.37975680000086093\n",
      "episode: 159   score: -125.0   memory length: 10000   epsilon: 0.3772693000008588\n",
      "episode: 160   score: -138.0   memory length: 10000   epsilon: 0.3745231000008564\n",
      "episode: 161   score: -200.0   memory length: 10000   epsilon: 0.370543100000853\n",
      "episode: 162   score: -115.0   memory length: 10000   epsilon: 0.36825460000085103\n",
      "episode: 163   score: -200.0   memory length: 10000   epsilon: 0.3642746000008476\n",
      "episode: 164   score: -200.0   memory length: 10000   epsilon: 0.3602946000008442\n",
      "episode: 165   score: -132.0   memory length: 10000   epsilon: 0.3576678000008419\n",
      "episode: 166   score: -200.0   memory length: 10000   epsilon: 0.3536878000008385\n",
      "episode: 167   score: -200.0   memory length: 10000   epsilon: 0.34970780000083507\n",
      "episode: 168   score: -144.0   memory length: 10000   epsilon: 0.3468422000008326\n",
      "episode: 169   score: -200.0   memory length: 10000   epsilon: 0.3428622000008292\n",
      "episode: 170   score: -155.0   memory length: 10000   epsilon: 0.3397777000008265\n",
      "episode: 171   score: -140.0   memory length: 10000   epsilon: 0.3369917000008241\n",
      "episode: 172   score: -161.0   memory length: 10000   epsilon: 0.33378780000082137\n",
      "episode: 173   score: -200.0   memory length: 10000   epsilon: 0.32980780000081794\n",
      "episode: 174   score: -126.0   memory length: 10000   epsilon: 0.3273004000008158\n",
      "episode: 175   score: -186.0   memory length: 10000   epsilon: 0.3235990000008126\n",
      "Saving model...\n",
      "episode: 176   score: -139.0   memory length: 10000   epsilon: 0.3208329000008102\n",
      "episode: 177   score: -200.0   memory length: 10000   epsilon: 0.3168529000008068\n",
      "episode: 178   score: -94.0   memory length: 10000   epsilon: 0.3149823000008052\n",
      "episode: 179   score: -96.0   memory length: 10000   epsilon: 0.31307190000080354\n",
      "episode: 180   score: -140.0   memory length: 10000   epsilon: 0.31028590000080114\n",
      "episode: 181   score: -200.0   memory length: 10000   epsilon: 0.3063059000007977\n",
      "episode: 182   score: -148.0   memory length: 10000   epsilon: 0.3033607000007952\n",
      "episode: 183   score: -200.0   memory length: 10000   epsilon: 0.29938070000079176\n",
      "episode: 184   score: -157.0   memory length: 10000   epsilon: 0.29625640000078907\n",
      "episode: 185   score: -200.0   memory length: 10000   epsilon: 0.29227640000078564\n",
      "episode: 186   score: -184.0   memory length: 10000   epsilon: 0.2886148000007825\n",
      "episode: 187   score: -154.0   memory length: 10000   epsilon: 0.28555020000077985\n",
      "episode: 188   score: -105.0   memory length: 10000   epsilon: 0.28346070000077805\n",
      "episode: 189   score: -122.0   memory length: 10000   epsilon: 0.28103290000077596\n",
      "episode: 190   score: -136.0   memory length: 10000   epsilon: 0.27832650000077364\n",
      "episode: 191   score: -190.0   memory length: 10000   epsilon: 0.2745455000007704\n",
      "episode: 192   score: -172.0   memory length: 10000   epsilon: 0.27112270000076744\n",
      "episode: 193   score: -166.0   memory length: 10000   epsilon: 0.2678193000007646\n",
      "episode: 194   score: -146.0   memory length: 10000   epsilon: 0.2649139000007621\n",
      "episode: 195   score: -163.0   memory length: 10000   epsilon: 0.2616702000007593\n",
      "episode: 196   score: -183.0   memory length: 10000   epsilon: 0.25802850000075617\n",
      "episode: 197   score: -155.0   memory length: 10000   epsilon: 0.2549440000007535\n",
      "episode: 198   score: -127.0   memory length: 10000   epsilon: 0.25241670000075134\n",
      "episode: 199   score: -159.0   memory length: 10000   epsilon: 0.24925260000074967\n",
      "episode: 200   score: -157.0   memory length: 10000   epsilon: 0.24612830000075134\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "N_EPISODES = 200\n",
    "\n",
    "for e in range(1, N_EPISODES + 1):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    # Action 0 (left), 1 (do nothing), 3 (declare fake_action to avoid doing nothing)\n",
    "    fake_action = 0\n",
    "\n",
    "    # Counter for the same action 7 times\n",
    "    action_count = 0\n",
    "\n",
    "    while not done:\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # Select an action in the current state and proceed to a step\n",
    "        action_count = action_count + 1\n",
    "                \n",
    "        if action_count == 7:\n",
    "            action = agent.get_action(state)\n",
    "            action_count = 0\n",
    "\n",
    "            if action == 0:\n",
    "                fake_action = 0\n",
    "            elif action == 1:\n",
    "                fake_action = 2\n",
    "\n",
    "        # Take 1 step with the selected action\n",
    "        next_state, reward, done, info = env.step(fake_action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # next_state = np.round(next_state, 3)\n",
    "        \n",
    "        # Give a penalty of -100 for actions that end an episode\n",
    "        # reward = reward if not done else -100\n",
    "        \n",
    "        # Save <s, a, r, s'> to replay memory\n",
    "        agent.replay_memory(state, fake_action, reward, next_state, done)\n",
    "        # Continue to learn every time step\n",
    "        agent.train_replay()\n",
    "        \n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            \n",
    "            # Copy the learning model for each episode to the target model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # For each episode, the time step where cartpole stood is plot\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            print('episode:', e, '  score:', score, '  memory length:', len(agent.memory), '  epsilon:', agent.epsilon)\n",
    "\n",
    "    if e % 25 == 0:\n",
    "        agent.save_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
