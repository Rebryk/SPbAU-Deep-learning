{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: fill empty spaces in the following agent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, name, state_size, action_size, learning_rate = 0.001):\n",
    "        self.name = name\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, [None, state_size])\n",
    "        self.y = tf.placeholder(tf.float32, [None, action_size])\n",
    "        \n",
    "        self.y_pred, self.optimizer = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            layer_1 = slim.fully_connected(self.X, 20)\n",
    "            layer_2 = slim.fully_connected(layer_1, 20)\n",
    "            y_pred = slim.fully_connected(layer_2, self.action_size, activation_fn=None)\n",
    "            loss = tf.losses.mean_squared_error(self.y, y_pred)\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
    "            return y_pred, optimizer\n",
    "    \n",
    "    def predict_reward(self, sess, state):\n",
    "        return sess.run(self.y_pred, feed_dict={self.X: state})[0]\n",
    "    \n",
    "    def predict_action(self, sess, state):\n",
    "        return np.argmax(self.predict_reward(sess, state))\n",
    "    \n",
    "    def train(self, sess, X, y):\n",
    "        sess.run(self.optimizer, feed_dict={self.X: X, self.y: y})\n",
    "    \n",
    "class DeepQAgent:\n",
    "    def __init__(self, state_size, action_size, render=True):\n",
    "        self.render = render\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / 50000\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        \n",
    "        # replay memory\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "        self.model = Model('model', state_size, action_size)\n",
    "        self.target_model = Model('target_model', state_size, action_size)\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.writer = tf.summary.FileWriter('./graphs', self.sess.graph)\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update your target model to the model you are currently learning at regular time intervals\"\"\"\n",
    "        \n",
    "        vars_1 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'target_model')\n",
    "        vars_2 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'model')\n",
    "\n",
    "        for x, y in zip(vars_1, vars_2):\n",
    "            self.sess.run(tf.assign(x, y.eval()))\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"The choice of action uses the epsilon-greedy policy for the current network.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return self.model.predict_action(self.sess, state)\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save <s, a, r, s'> to replay_memory\"\"\"\n",
    "        if action == 2:\n",
    "            action = 1\n",
    "            \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "    \n",
    "    def train_replay(self):\n",
    "        \"\"\"Random sampling of batch_size samples from replay memory\"\"\"\n",
    "        \n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.action_size))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            target = self.model.predict_reward(self.sess, state)\n",
    "\n",
    "            # As in queuing, it gets the maximum Q Value at s'. However, it is imported from the target model.\n",
    "            if done:\n",
    "                # target[action] = reward\n",
    "                target[action] = 1000\n",
    "            else:\n",
    "                max_q = np.max(self.target_model.predict_reward(self.sess, next_state))\n",
    "                target[action] = reward + self.discount_factor * max_q\n",
    "            \n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        self.model.train(self.sess, update_input, update_target)\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            print('Trying to restore last checkpoint...')\n",
    "            last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=\"./checkpoints/\")\n",
    "            self.saver.restore(self.sess, save_path=last_chk_path)\n",
    "            print('Restored checkpoint from:', last_chk_path)\n",
    "        except:\n",
    "            print('Failed to restore checkpoint. Initializing variables instead.')\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def save_model(self, name):\n",
    "        print('Saving model...')\n",
    "        self.saver.save(self.sess, save_path='checkpoints/' + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0] # should be equal 2\n",
    "\n",
    "ACTION_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "agent = DeepQAgent(state_size, ACTION_SIZE)\n",
    "agent.load_model()\n",
    "scores, episodes = [], []\n",
    "\n",
    "N_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(N_EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    # Action 0 (left), 1 (do nothing), 3 (declare fake_action to avoid doing nothing)\n",
    "    fake_action = 0\n",
    "\n",
    "    # Counter for the same action 6 times\n",
    "    action_count = 0\n",
    "\n",
    "    while not done:\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # Select an action in the current state and proceed to a step\n",
    "        action_count = action_count + 1\n",
    "                \n",
    "        if action_count == 6:\n",
    "            action = agent.get_action(state)\n",
    "            action_count = 0\n",
    "\n",
    "            if action == 0:\n",
    "                fake_action = 0\n",
    "            elif action == 1:\n",
    "                fake_action = 2\n",
    "\n",
    "        # Take 1 step with the selected action\n",
    "        next_state, reward, done, info = env.step(fake_action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        next_state = np.round(next_state, 3)\n",
    "        \n",
    "        # Give a penalty of -100 for actions that end an episode\n",
    "        # reward = reward if not done else -100\n",
    "        \n",
    "        # Save <s, a, r, s'> to replay memory\n",
    "        agent.replay_memory(state, fake_action, reward, next_state, done)\n",
    "        # Continue to learn every time step\n",
    "        agent.train_replay()\n",
    "        \n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            # Copy the learning model for each episode to the target model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # For each episode, the time step where cartpole stood is plot\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            print('episode:', e, '  score:', score, '  memory length:', len(agent.memory), '  epsilon:', agent.epsilon)\n",
    "\n",
    "    # Save model for every 200 episodes\n",
    "    if e % 50 == 0:\n",
    "        agent.save_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
